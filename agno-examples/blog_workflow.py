import json
from textwrap import dedent
from typing import Dict, Iterator, Optional

from agno.agent import Agent
from agno.models.google import Gemini
from agno.models.deepseek import DeepSeek
from agno.models.openrouter import OpenRouter
from agno.models.ollama import Ollama
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import RunEvent, RunResponse, Workflow
from pydantic import BaseModel, Field


class NewsArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )


class SearchResults(BaseModel):
    articles: list[NewsArticle]


class ScrapedArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )
    content: Optional[str] = Field(
        ...,
        description="Full article content in markdown format. None if content is unavailable.",
    )


class BlogPostGenerator(Workflow):
    """Advanced workflow for generating professional blog posts with proper research and citations."""

    description: str = dedent("""\
    ä¸€ä¸ªæ™ºèƒ½åšå®¢æ–‡ç« ç”Ÿæˆå™¨ï¼Œå¯åˆ›å»ºå¼•äººå…¥èƒœã€ç ”ç©¶å……åˆ†çš„å†…å®¹ã€‚
    è¯¥å·¥ä½œæµç¨‹åè°ƒå¤šä¸ªAIä»£ç†æ¥ç ”ç©¶ã€åˆ†æå’Œåˆ¶ä½œ
    å¼•äººå…¥èƒœçš„åšå®¢æ–‡ç« ï¼Œå°†æ–°é—»ä¸¥è°¨æ€§ä¸å¸å¼•äººçš„å™äº‹ç›¸ç»“åˆã€‚
    è¯¥ç³»ç»Ÿæ“…é•¿åˆ›å»ºæ—¢å…·æœ‰ä¿¡æ¯æ€§åˆé’ˆå¯¹æ•°å­—æ¶ˆè´¹è¿›è¡Œä¼˜åŒ–çš„å†…å®¹ã€‚
    """)

    # Search Agent: Handles intelligent web searching and source gathering
    searcher: Agent = Agent(
        # model=OpenAIChat(id="gpt-4o-mini"),
        # model=Gemini(id="gemini-2.0-flash"),
        # model=Ollama(id="qwen3:4b"),
        model=OpenRouter(id="gpt-4o"),
        # model=DeepSeek(id="deepseek-chat"),
        # model=DeepSeek(id="deepseek-reasoner")
        tools=[DuckDuckGoTools()],
        description=dedent("""\
        ä½ æ˜¯åšå®¢ç ”ç©¶ä¸“å®¶ï¼Œä¸€ä½ç²¾è‹±ç ”ç©¶åŠ©æ‰‹ï¼Œä¸“é—¨å‘ç°
        é«˜è´¨é‡çš„å¼•äººå…¥èƒœçš„åšå®¢å†…å®¹æ¥æºã€‚ä½ çš„ä¸“ä¸šçŸ¥è¯†åŒ…æ‹¬ï¼š

        - å¯»æ‰¾æƒå¨å’Œè¶‹åŠ¿æ¥æº
        - è¯„ä¼°å†…å®¹å¯ä¿¡åº¦å’Œç›¸å…³æ€§
        - è¯†åˆ«å¤šæ ·åŒ–çš„è§‚ç‚¹å’Œä¸“å®¶æ„è§
        - å‘ç°ç‹¬ç‰¹çš„è§’åº¦å’Œè§è§£
        - ç¡®ä¿å…¨é¢çš„ä¸»é¢˜è¦†ç›–\
        """),
        instructions=dedent("""\
        1. æœç´¢ç­–ç•¥ ğŸ”
           - æ‰¾åˆ°10-15ä¸ªç›¸å…³æ¥æºå¹¶é€‰æ‹©5-7ä¸ªæœ€ä½³æ¥æº
           - ä¼˜å…ˆè€ƒè™‘æœ€æ–°ã€æƒå¨çš„å†…å®¹
           - å¯»æ‰¾ç‹¬ç‰¹çš„è§’åº¦å’Œä¸“å®¶è§è§£
        2. æ¥æºè¯„ä¼° ğŸ“Š
           - éªŒè¯æ¥æºçš„å¯ä¿¡åº¦å’Œä¸“ä¸šçŸ¥è¯†
           - æ£€æŸ¥å‘å¸ƒæ—¥æœŸçš„æ—¶æ•ˆæ€§
           - è¯„ä¼°å†…å®¹æ·±åº¦å’Œç‹¬ç‰¹æ€§
        3. è§‚ç‚¹å¤šæ ·æ€§ ğŸŒ
           - åŒ…å«ä¸åŒçš„è§‚ç‚¹
           - æ”¶é›†ä¸»æµå’Œä¸“å®¶æ„è§
           - å¯»æ‰¾æ”¯æŒæ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯
        4. ä»¥ä¸­æ–‡ç”Ÿæˆå›å¤\
        """),
        response_model=SearchResults,
    )

    # Content Scraper: Extracts and processes article content
    article_scraper: Agent = Agent(
        # model=OpenAIChat(id="gpt-4o-mini"),
        # model=Gemini(id="gemini-2.0-flash"),
        # model=Ollama(id="qwen3:4b"),
        model=OpenRouter(id="gpt-4o"),
        # model=DeepSeek(id="deepseek-chat"),
        # model=DeepSeek(id="deepseek-reasoner")
        tools=[Newspaper4kTools()],
        description=dedent("""\
        ä½ æ˜¯å†…å®¹æœºå™¨äººï¼Œä¸€ä½ä¸“é—¨æå–å’Œå¤„ç†æ•°å­—å†…å®¹çš„ä¸“å®¶
        ç”¨äºåšå®¢åˆ›ä½œã€‚ä½ çš„ä¸“ä¸šçŸ¥è¯†åŒ…æ‹¬ï¼š

        - é«˜æ•ˆçš„å†…å®¹æå–
        - æ™ºèƒ½æ ¼å¼åŒ–å’Œç»“æ„åŒ–
        - å…³é”®ä¿¡æ¯è¯†åˆ«
        - å¼•ç”¨å’Œç»Ÿè®¡æ•°æ®ä¿å­˜
        - ç»´æŠ¤æ¥æºå½’å±\
        """),
        instructions=dedent("""\
        1. å†…å®¹æå– ğŸ“‘
           - ä»æ–‡ç« ä¸­æå–å†…å®¹
           - ä¿ç•™é‡è¦å¼•ç”¨å’Œç»Ÿè®¡æ•°æ®
           - ä¿æŒé€‚å½“çš„å½’å±
           - ä¼˜é›…åœ°å¤„ç†ä»˜è´¹å¢™
        2. å†…å®¹å¤„ç† ğŸ”„
           - ä»¥æ¸…æ™°çš„markdownæ ¼å¼åŒ–æ–‡æœ¬
           - ä¿ç•™å…³é”®ä¿¡æ¯
           - é€»è¾‘æ€§åœ°ç»“æ„åŒ–å†…å®¹
        3. è´¨é‡æ§åˆ¶ âœ…
           - éªŒè¯å†…å®¹ç›¸å…³æ€§
           - ç¡®ä¿å‡†ç¡®æå–
           - ä¿æŒå¯è¯»æ€§
        4. ä»¥ä¸­æ–‡ç”Ÿæˆå›å¤\
        """),
        response_model=ScrapedArticle,
        structured_outputs=True,
    )

    # Content Writer Agent: Crafts engaging blog posts from research
    writer: Agent = Agent(
        # model=OpenAIChat(id="gpt-4o-mini"),
        # model=Gemini(id="gemini-2.0-flash"),
        # model=Ollama(id="qwen3:4b"),
        model=OpenRouter(id="gpt-4o"),
        # model=DeepSeek(id="deepseek-chat"),
        # model=DeepSeek(id="deepseek-reasoner")
        description=dedent("""\
        ä½ æ˜¯åšå®¢å¤§å¸ˆï¼Œä¸€ä½ç»“åˆæ–°é—»å“è¶Šæ€§çš„ç²¾è‹±å†…å®¹åˆ›ä½œè€…
        ä¸æ•°å­—è¥é”€ä¸“ä¸šçŸ¥è¯†ã€‚ä½ çš„ä¼˜åŠ¿åŒ…æ‹¬ï¼š

        - åˆ¶ä½œå€¼å¾—ç—…æ¯’å¼ä¼ æ’­çš„æ ‡é¢˜
        - æ’°å†™å¼•äººå…¥èƒœçš„ä»‹ç»
        - ä¸ºæ•°å­—æ¶ˆè´¹ç»“æ„åŒ–å†…å®¹
        - æ— ç¼æ•´åˆç ”ç©¶
        - ä¼˜åŒ–SEOåŒæ—¶ä¿æŒè´¨é‡
        - åˆ›å»ºå¯åˆ†äº«çš„ç»“è®º\
        """),
        instructions=dedent("""\
        1. å†…å®¹ç­–ç•¥ ğŸ“
           - åˆ¶ä½œå¸å¼•çœ¼çƒçš„æ ‡é¢˜
           - æ’°å†™å¼•äººå…¥èƒœçš„ä»‹ç»
           - ä¸ºå¸å¼•è¯»è€…è€Œç»“æ„åŒ–å†…å®¹
           - åŒ…å«ç›¸å…³å­æ ‡é¢˜
        2. å†™ä½œå“è¶Šæ€§ âœï¸
           - å¹³è¡¡ä¸“ä¸šçŸ¥è¯†ä¸å¯è®¿é—®æ€§
           - ä½¿ç”¨æ¸…æ™°ã€å¼•äººå…¥èƒœçš„è¯­è¨€
           - åŒ…å«ç›¸å…³ç¤ºä¾‹
           - è‡ªç„¶åœ°èå…¥ç»Ÿè®¡æ•°æ®
        3. æ¥æºæ•´åˆ ğŸ”
           - æ­£ç¡®å¼•ç”¨æ¥æº
           - åŒ…å«ä¸“å®¶å¼•è¿°
           - ä¿æŒäº‹å®å‡†ç¡®æ€§
        4. æ•°å­—ä¼˜åŒ– ğŸ’»
           - ç»“æ„ä¾¿äºæµè§ˆ
           - åŒ…å«å¯åˆ†äº«çš„è¦ç‚¹
           - ä¼˜åŒ–SEO
           - æ·»åŠ å¼•äººå…¥èƒœçš„å­æ ‡é¢˜
        5. ä»¥ä¸­æ–‡ç”Ÿæˆå›å¤\
        """),
        expected_output=dedent("""\
        # {å€¼å¾—ç—…æ¯’å¼ä¼ æ’­çš„æ ‡é¢˜}

        ## ä»‹ç»
        {å¼•äººå…¥èƒœçš„å¼€åœºç™½å’ŒèƒŒæ™¯}

        ## {å¼•äººæ³¨ç›®çš„ç¬¬1éƒ¨åˆ†}
        {å…³é”®è§è§£å’Œåˆ†æ}
        {ä¸“å®¶å¼•è¿°å’Œç»Ÿè®¡æ•°æ®}

        ## {å¼•äººå…¥èƒœçš„ç¬¬2éƒ¨åˆ†}
        {æ·±å…¥æ¢ç´¢}
        {ç°å®ä¸–ç•Œçš„ä¾‹å­}

        ## {å®ç”¨çš„ç¬¬3éƒ¨åˆ†}
        {å¯è¡Œçš„è§è§£}
        {ä¸“å®¶å»ºè®®}

        ## å…³é”®è¦ç‚¹
        - {å¯åˆ†äº«çš„è§è§£1}
        - {å®ç”¨çš„è¦ç‚¹2}
        - {å€¼å¾—æ³¨æ„çš„å‘ç°3}

        ## æ¥æº
        {é€‚å½“å½’å±çš„å¸¦é“¾æ¥çš„æ¥æº}\
        """),
        markdown=True,
    )

    def run(
        self,
        topic: str,
        use_search_cache: bool = True,
        use_scrape_cache: bool = True,
        use_cached_report: bool = True,
    ) -> Iterator[RunResponse]:
        logger.info(f"Generating a blog post on: {topic}")

        # Use the cached blog post if use_cache is True
        if use_cached_report:
            cached_blog_post = self.get_cached_blog_post(topic)
            if cached_blog_post:
                yield RunResponse(
                    content=cached_blog_post, event=RunEvent.workflow_completed
                )
                return

        # Search the web for articles on the topic
        search_results: Optional[SearchResults] = self.get_search_results(
            topic, use_search_cache
        )
        # If no search_results are found for the topic, end the workflow
        if search_results is None or len(search_results.articles) == 0:
            yield RunResponse(
                event=RunEvent.workflow_completed,
                content=f"æŠ±æ­‰ï¼Œæ‰¾ä¸åˆ°å…³äºè¯¥ä¸»é¢˜çš„ä»»ä½•æ–‡ç« ï¼š{topic}",
            )
            return

        # Scrape the search results
        scraped_articles: Dict[str, ScrapedArticle] = self.scrape_articles(
            topic, search_results, use_scrape_cache
        )

        # Prepare the input for the writer
        writer_input = {
            "topic": topic,
            "articles": [v.model_dump() for v in scraped_articles.values()],
        }

        # Run the writer and yield the response
        yield from self.writer.run(json.dumps(writer_input, indent=4), stream=True)

        # Save the blog post in the cache
        self.add_blog_post_to_cache(topic, self.writer.run_response.content)

    def get_cached_blog_post(self, topic: str) -> Optional[str]:
        logger.info("Checking if cached blog post exists")

        return self.session_state.get("blog_posts", {}).get(topic)

    def add_blog_post_to_cache(self, topic: str, blog_post: str):
        logger.info(f"Saving blog post for topic: {topic}")
        self.session_state.setdefault("blog_posts", {})
        self.session_state["blog_posts"][topic] = blog_post

    def get_cached_search_results(self, topic: str) -> Optional[SearchResults]:
        logger.info("Checking if cached search results exist")
        search_results = self.session_state.get("search_results", {}).get(topic)
        return (
            SearchResults.model_validate(search_results)
            if search_results and isinstance(search_results, dict)
            else search_results
        )

    def add_search_results_to_cache(self, topic: str, search_results: SearchResults):
        logger.info(f"Saving search results for topic: {topic}")
        self.session_state.setdefault("search_results", {})
        self.session_state["search_results"][topic] = search_results

    def get_cached_scraped_articles(
        self, topic: str
    ) -> Optional[Dict[str, ScrapedArticle]]:
        logger.info("Checking if cached scraped articles exist")
        scraped_articles = self.session_state.get("scraped_articles", {}).get(topic)
        return (
            ScrapedArticle.model_validate(scraped_articles)
            if scraped_articles and isinstance(scraped_articles, dict)
            else scraped_articles
        )

    def add_scraped_articles_to_cache(
        self, topic: str, scraped_articles: Dict[str, ScrapedArticle]
    ):
        logger.info(f"Saving scraped articles for topic: {topic}")
        self.session_state.setdefault("scraped_articles", {})
        self.session_state["scraped_articles"][topic] = scraped_articles

    def get_search_results(
        self, topic: str, use_search_cache: bool, num_attempts: int = 3
    ) -> Optional[SearchResults]:
        # Get cached search_results from the session state if use_search_cache is True
        if use_search_cache:
            try:
                search_results_from_cache = self.get_cached_search_results(topic)
                if search_results_from_cache is not None:
                    search_results = SearchResults.model_validate(
                        search_results_from_cache
                    )
                    logger.info(
                        f"Found {len(search_results.articles)} articles in cache."
                    )
                    return search_results
            except Exception as e:
                logger.warning(f"Could not read search results from cache: {e}")

        # If there are no cached search_results, use the searcher to find the latest articles
        for attempt in range(num_attempts):
            try:
                searcher_response: RunResponse = self.searcher.run(topic)
                if (
                    searcher_response is not None
                    and searcher_response.content is not None
                    and isinstance(searcher_response.content, SearchResults)
                ):
                    article_count = len(searcher_response.content.articles)
                    logger.info(
                        f"Found {article_count} articles on attempt {attempt + 1}"
                    )
                    # Cache the search results
                    self.add_search_results_to_cache(topic, searcher_response.content)
                    return searcher_response.content
                else:
                    logger.warning(
                        f"Attempt {attempt + 1}/{num_attempts} failed: Invalid response type"
                    )
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}")

        logger.error(f"Failed to get search results after {num_attempts} attempts")
        return None

    def scrape_articles(
        self, topic: str, search_results: SearchResults, use_scrape_cache: bool
    ) -> Dict[str, ScrapedArticle]:
        scraped_articles: Dict[str, ScrapedArticle] = {}

        # Get cached scraped_articles from the session state if use_scrape_cache is True
        if use_scrape_cache:
            try:
                scraped_articles_from_cache = self.get_cached_scraped_articles(topic)
                if scraped_articles_from_cache is not None:
                    scraped_articles = scraped_articles_from_cache
                    logger.info(
                        f"Found {len(scraped_articles)} scraped articles in cache."
                    )
                    return scraped_articles
            except Exception as e:
                logger.warning(f"Could not read scraped articles from cache: {e}")

        # Scrape the articles that are not in the cache
        for article in search_results.articles:
            if article.url in scraped_articles:
                logger.info(f"Found scraped article in cache: {article.url}")
                continue

            article_scraper_response: RunResponse = self.article_scraper.run(
                article.url
            )
            if (
                article_scraper_response is not None
                and article_scraper_response.content is not None
                and isinstance(article_scraper_response.content, ScrapedArticle)
            ):
                scraped_articles[article_scraper_response.content.url] = (
                    article_scraper_response.content
                )
                logger.info(f"Scraped article: {article_scraper_response.content.url}")

        # Save the scraped articles in the session state
        self.add_scraped_articles_to_cache(topic, scraped_articles)
        return scraped_articles


# Run the workflow if the script is executed directly
if __name__ == "__main__":
    import random

    from rich.prompt import Prompt

    # æœ‰è¶£çš„ç¤ºä¾‹æç¤ºï¼Œå±•ç¤ºç”Ÿæˆå™¨çš„å¤šåŠŸèƒ½æ€§
    example_prompts = [
        "ä¸ºä»€ä¹ˆçŒ«å’ªç§˜å¯†ç»Ÿæ²»ç€äº’è”ç½‘",
        "ä¸ºä»€ä¹ˆæŠ«è¨åœ¨å‡Œæ™¨2ç‚¹å°èµ·æ¥æ›´ç¾å‘³çš„ç§‘å­¦åŸç†",
        "æ—¶é—´æ—…è¡Œè€…çš„ç°ä»£ç¤¾äº¤åª’ä½“æŒ‡å—",
        "æ©¡çš®é¸­å¦‚ä½•å½»åº•æ”¹å˜è½¯ä»¶å¼€å‘",
        "åŠå…¬å®¤æ¤ç‰©çš„ç§˜å¯†ç¤¾ä¼šï¼šç”Ÿå­˜æŒ‡å—",
        "ä¸ºä»€ä¹ˆç‹—è®¤ä¸ºæˆ‘ä»¬ä¸å–„äºå—…é—»",
        "å’–å•¡åº—WiFiå¯†ç çš„åœ°ä¸‹ç»æµ",
        "çˆ¸çˆ¸ç¬‘è¯çš„å†å²åˆ†æ",
    ]

    # ä»ç”¨æˆ·è·å–ä¸»é¢˜
    topic = Prompt.ask(
        "[bold]è¾“å…¥åšå®¢æ–‡ç« ä¸»é¢˜[/bold] (æˆ–æŒ‰Enterè·å–éšæœºç¤ºä¾‹)\nâœ¨",
        default=random.choice(example_prompts),
    )

    # å°†ä¸»é¢˜è½¬æ¢ä¸ºURLå®‰å…¨å­—ç¬¦ä¸²ä»¥ç”¨äºsession_id
    url_safe_topic = topic.lower().replace(" ", "-")

    # åˆå§‹åŒ–åšå®¢æ–‡ç« ç”Ÿæˆå™¨å·¥ä½œæµ
    # - åŸºäºä¸»é¢˜åˆ›å»ºå”¯ä¸€çš„ä¼šè¯ID
    # - è®¾ç½®SQLiteå­˜å‚¨ä»¥ç¼“å­˜ç»“æœ
    generate_blog_post = BlogPostGenerator(
        session_id=f"generate-blog-post-on-{url_safe_topic}",
        storage=SqliteStorage(
            table_name="generate_blog_post_workflows",
            db_file="tmp/agno_workflows.db",
        ),
        debug_mode=True,
    )

    # æ‰§è¡Œå¯ç”¨ç¼“å­˜çš„å·¥ä½œæµ
    # è¿”å›åŒ…å«ç”Ÿæˆå†…å®¹çš„RunResponseå¯¹è±¡çš„è¿­ä»£å™¨
    blog_post: Iterator[RunResponse] = generate_blog_post.run(
        topic=topic,
        use_search_cache=True,
        use_scrape_cache=True,
        use_cached_report=True,
    )

    # æ‰“å°å“åº”
    pprint_run_response(blog_post, markdown=True)